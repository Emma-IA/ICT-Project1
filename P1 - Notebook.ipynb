{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Project 1 - Information measures\n",
    "\n",
    "The goal of this first project is to get accustomed to the information and uncertainty measures. We ask you to write a brief report (pdf format) collecting your answers to the different questions. All codes must be written in Python inside this Jupyter Notebook. No other code file will be accepted. Note that you can not change the content of locked cells or import any extra Python library than the ones already imported (numpy and pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Implementation\n",
    "\n",
    "In this project, you will need to use information measures to answer several questions. Therefore, in this first part, you are asked to write several functions that implement some of the main measures seen in the first theoretical lectures. Remember that you need to fill in this Jupyter Notebook to answer these questions. Pay particular attention to the required output format of each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# [Locked Cell] You can not import any extra Python library in this Notebook.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Write a function entropy that computes the entropy $\\mathcal{H(X)}$ of a random variable $\\mathcal{X}$ from its probability distribution $P_\\mathcal{X} = (p_1, p_2, . . . , p_n)$. Give the mathematical formula that you are using and explain the key parts of your implementation. Intuitively, what is measured by the entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(Px):\n",
    "    \n",
    "    n=len(Px)\n",
    "    H=0\n",
    "    for i in range(n):\n",
    "        if Px[i]>0:\n",
    "            H+=-Px[i]*log(Px[i])\n",
    "    return H\n",
    "    \"\"\"\n",
    "    Computes the entropy from the marginal probability distribution. \n",
    "    Arguments:\n",
    "    ----------\n",
    "    - Px :  Marginal probability distribution of the random \n",
    "            variable X in a numpy array where Px[i]=P(X=i)\n",
    "    Return:\n",
    "    -------\n",
    "    - The entropy of X (H(X)) as a number (integer, float or double).\n",
    "    \"\"\"    \n",
    "    n=len(Px)\n",
    "    H=0\n",
    "    for i in range(n):\n",
    "        H+=-Px[i]*log(Px[i])\n",
    "    return H\n",
    "\n",
    "print (entropy([0.1,0.2,0.3,0.04]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Write a function joint_entropy that computes the joint entropy $\\mathcal{H(X,Y)}$ of two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Compare the entropy and joint_entropy functions (and their corresponding formulas), what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_entropy(Pxy):\n",
    "    \n",
    "    (m,n)=np.shape(Pxy)\n",
    "    H=0\n",
    "    for i in range (m):\n",
    "        for j in range (n):\n",
    "            if Pxy[i][j]>0:\n",
    "                H+=-Pxy[i][j]*log(Pxy[i][j])\n",
    "    return H\n",
    "            \n",
    "\"\"\"def joint_entropy2(Pxy):\n",
    "    \n",
    "    (m,n)=np.shape(Pxy)\n",
    "    H=0\n",
    "    for i in range (m):\n",
    "        H+=entropy(Pxy[i])\n",
    "    return H\"\"\"    \n",
    "\n",
    "print (joint_entropy([[0.01,0.02,0.03],[0.01,0.02,0.03]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Write a function conditional_entropy that computes the conditional entropy $\\mathcal{H(X|Y)}$ of a discrete random variable $\\mathcal{X}$ given another discrete random variable $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Describe an equivalent way of computing that quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(Pxy):\n",
    "    m,n=np.shape(Pxy)\n",
    "    Py=np.zeros(n)\n",
    "    for i in range (n):\n",
    "        for j in range (m):\n",
    "            Py[i]+=Pxy[j][i]\n",
    "    return joint_entropy(Pxy) - entropy(Py)\n",
    "\n",
    "conditional_entropy([[1,2,3],[1,2,3]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Write a function mutual_information that computes the mutual information $\\mathcal{I(X;Y)}$ between two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$ . Give the mathematical formula that you are using and explain the key parts of your implementation. What can you deduce from the mutual information $\\mathcal{I(X;Y)}$ on the relationship between $\\mathcal{X}$ and $\\mathcal{Y}$? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(Pxy):\n",
    "    \n",
    "    m,n=np.shape(Pxy)\n",
    "    Px=np.sum(Pxy, axis=(1))\n",
    "    Py=np.sum(Pxy, axis=(0))\n",
    "    return entropy(Px)+entropy(Py)-conditional_entropy(Pxy)\n",
    "\n",
    "mutual_information([[0.1,0.2,0.1],[0.1,0.2,0.3]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Let $\\mathcal{X}$, $\\mathcal{Y}$ and $\\mathcal{Z}$ be three discrete random variables. Write the functions cond_joint_entropy and cond_mutual_information that respectively compute $\\mathcal{H(X,Y|Z)}$ and $\\mathcal{I(X;Y|Z)}$ of two discrete random variable $\\mathcal{X}$, $\\mathcal{Y}$ given another discrete random variable $\\mathcal{Z}$ from their joint probability distribution $P_\\mathcal{X,Y,Z}$. Give the mathematical formulas that you are using and explain the key parts of your implementation.\n",
    "Suggestion: Observe the mathematical definitions of these quantities and think how you could derive them from the joint entropy and the mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_joint_entropy(Pxyz):\n",
    "    \n",
    "    (m,n,o)=np.shape(Pxyz)\n",
    "    H=0\n",
    "    for i in range (m):\n",
    "        for j in range (n):\n",
    "            for o in range (o):\n",
    "                if Pxyz[i][j][o]>0:\n",
    "                    H+=-Pxyz[i][j][o]*log(Pxyz[i][j][o])\n",
    "    return H\n",
    "\n",
    "def cond_joint_entropy(Pxyz):\n",
    "    Pxz=np.sum(Pxyz, axis=(1))\n",
    "    return conditional_entropy(Pxz)+multi_joint_entropy(Pxyz) -joint_entropy(Pxz)\n",
    "\n",
    "#return joint_entropy(Pxz) - \n",
    "                        \n",
    "cond_joint_entropy([[[0.01,0.02,0.01],[0.02,0.02,0.01]],[[0.01,0.01,0.01],[0.02,0.06,0.01]]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_mutual_information(Pxyz):\n",
    "    Pxz=np.sum(Pxyz, axis=(1))\n",
    "    Pyz=np.sum(Pxyz, axis=(0))\n",
    "    return conditional_entropy(Pxz)-(cond_joint_entropy(Pxyz)-conditional_entropy(Pyz))\n",
    "cond_mutual_information([[[0.01,0.02],[0.02,0.02],[0.03,0.03]],[[0.01,0.01],[0.02,0.06],[0.01,0.03]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# [Locked Cell] Evaluation of your functions by the examiner. \n",
    "# You don't have access to the evaluation, this will be done by the examiner.\n",
    "# Therefore, this cell will return nothing for the students.\n",
    "import os\n",
    "if os.path.isfile(\"private_evaluation.py\"):\n",
    "    from private_evaluation import unit_tests\n",
    "    unit_tests(entropy, joint_entropy, conditional_entropy, mutual_information, cond_joint_entropy, cond_mutual_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Weather forecasting\n",
    "\n",
    "You may create cells below to answer the different questions related to weather forecasting. Unlike in the first part (Implementation), you are free to define as many cells as you need below to answer the different questions. Try to be structured and clear in your code (comment it if necessary). Note that you have to answer the questions in the pdf report, including the numbers you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write your code here or in other cells below (you may delete this comment)\n",
    "data=pd.read_csv(\"weather_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "\n",
    "def list_features(Px):\n",
    "    list=[]\n",
    "    for w in Px:\n",
    "        if w not in list:\n",
    "            list.append(w)\n",
    "    return list\n",
    "\n",
    "def count_strings(Px): #returns proportionnality of each occurency in a column + the number of occurencies   \n",
    "    return Px.value_counts()\n",
    "\n",
    "    \n",
    "def display_entropy(data):#returns the dictionnary corresponding\n",
    "    dico_final=dict()\n",
    "    for feature in data.columns:\n",
    "        values=list_features(data[feature])\n",
    "        C=count_strings(data[feature])\n",
    "        P=pd.Series(C)/5000\n",
    "        D={feature:[entropy(P),len(values)]}\n",
    "        dico_final.update(D)\n",
    "    return dico_final\n",
    "\n",
    "print(count_strings(data[\"temperature\"]))\n",
    "display_entropy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "\n",
    "def joint_probability(data, feature1, feature2):\n",
    "    dataX=data[feature1]\n",
    "    dataY=data[feature2]\n",
    "    X=list_features(dataX)\n",
    "    Y=list_features(dataY)\n",
    "    size_x=len(X)\n",
    "    size_y=len(Y)\n",
    "    cond_list=np.zeros(size_x*size_y)\n",
    "    list=[]\n",
    "    for x in (X):\n",
    "        for y in (Y):\n",
    "            list.append((x,y))\n",
    "    for i in range (len(data)):\n",
    "        xi=dataX[i]\n",
    "        yi=dataY[i]\n",
    "        j=list.index((xi,yi))\n",
    "        cond_list[j]+=1\n",
    "    cond_list=cond_list/len(data)\n",
    "    cond_list=cond_list.reshape(size_x, size_y)\n",
    "    return cond_list\n",
    "            \n",
    "            \n",
    "P_next_pressure=joint_probability(data, \"next_day_rain\", \"air_pressure\")\n",
    "P_next_direction=joint_probability(data, \"next_day_rain\", \"wind_direction\")\n",
    "P_next_same=joint_probability(data, \"next_day_rain\", \"same_day_rain\")\n",
    "P_next_next=joint_probability(data, \"next_day_rain\", \"next_day_rain\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in data.columns:\n",
    "    print ('entropy of next_day_rain given', feature,\":\", joint_entropy(joint_probability(data, \"next_day_rain\", feature)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8\n",
    "\n",
    "P_humid_speed=joint_probability(data, \"relative_humidity\", \"wind_speed\")\n",
    "P_month_temp=joint_probability(data, \"month\", \"temperature\")\n",
    "print(\"mutual information between relative\\_humidity and wind\\_speed :\", mutual_information(P_humid_speed))\n",
    "print(\"mutual information between month and temperature :\", mutual_information(P_month_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9\n",
    "\n",
    "##en toute logique: air pressure .. soit max mutual info et min conditional entropy ... réfléchir à la justification\n",
    "\n",
    "def max_mutual_info(data):\n",
    "    list=[]\n",
    "    for feature in data.columns:\n",
    "        if feature != \"next_day_rain\":\n",
    "            list.append((mutual_information(joint_probability(data, \"next_day_rain\", feature)), feature))\n",
    "    return max(list)\n",
    "\n",
    "print(max_mutual_info(data))\n",
    "\n",
    "def min_conditional_entropy(data):\n",
    "    list=[]\n",
    "    for feature in data.columns:\n",
    "        if feature != \"next_day_rain\":\n",
    "            list.append((conditional_entropy(joint_probability(data, \"next_day_rain\", feature)), feature))\n",
    "    return min(list)\n",
    "\n",
    "print(min_conditional_entropy(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 10\n",
    "data_modif=data[data['next_day_rain'].isin([\"drizzle\",\"deluge\"])]\n",
    "data_modif\n",
    "#sans dry : quelle mutuelle info ou conditional entropy serait utile ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suite question 10 : ne marche pas, I don't know why\n",
    "\n",
    "dataX=data_modif['next_day_rain']\n",
    "dataY=data_modif[\"air_pressure\"]\n",
    "print(dataY[6])\n",
    "X=list_features(dataX)\n",
    "print(dataX[20])\n",
    "Y=list_features(dataY)\n",
    "size_x=len(X)\n",
    "size_y=len(Y)\n",
    "cond_list=np.zeros(size_x*size_y)\n",
    "list=[]\n",
    "for x in (X):\n",
    "    for y in (Y):\n",
    "        list.append((x,y))\n",
    "print(list)\n",
    "for i in range (len(data_modif)):\n",
    "    print(dataY[i])\n",
    "    print(dataX[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 11\n",
    "\n",
    "#erreurs, valeurs négatives ... \n",
    "\n",
    "def multi_joint_probability(data, feature1, feature2, feature3):\n",
    "    dataX=data[feature1]\n",
    "    dataY=data[feature2]\n",
    "    dataZ=data[feature3]\n",
    "    X=list_features(dataX)\n",
    "    Y=list_features(dataY)\n",
    "    Z=list_features(dataZ)\n",
    "    size_x=len(X)\n",
    "    size_y=len(Y)\n",
    "    size_z=len(Z)\n",
    "    cond_list=np.zeros(size_x*size_y*size_z)\n",
    "    list=[]\n",
    "    for x in (X):\n",
    "        for y in (Y):\n",
    "            for z in (Z):\n",
    "                list.append((x,y,z))\n",
    "    for i in range (len(data)):\n",
    "                xi=dataX[i]\n",
    "                yi=dataY[i]\n",
    "                zi=dataZ[i]\n",
    "                j=list.index((xi,yi, zi))\n",
    "                cond_list[j]+=1\n",
    "    cond_list=cond_list/len(data)\n",
    "    cond_list=cond_list.reshape(size_x, size_y, size_z)\n",
    "    return cond_list\n",
    "\n",
    "def min_cond_mutual_info(data):\n",
    "    list=[]\n",
    "    for feature in data.columns:\n",
    "        if feature != \"next_day_rain\":\n",
    "            list.append((cond_mutual_information(multi_joint_probability(data, feature,\"next_day_rain\", \"temperature\")), feature))\n",
    "    return list\n",
    "\n",
    "print(min_cond_mutual_info(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words at the start = 11881376\n",
      "Entropy = 23.50219859070546\n",
      "Number of different words after first guess = 234256\n",
      "Entropy = 17.83772647454919\n",
      "Number of different words after second guess = 15623\n",
      "Entropy = 13.931383892539374\n",
      "Entropy of each cell\n",
      "{'a': 0, 'b': 0, 'c': 0, 'd': 0, 'e': 0, 'f': 0, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n",
      "Cell 0 = 3.582729760737487\n",
      "{'a': 0, 'b': 0, 'c': 0, 'd': 0, 'e': 0, 'f': 0, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n",
      "Cell 1 = 0.0\n",
      "{'a': 0, 'b': 0, 'c': 0, 'd': 0, 'e': 0, 'f': 0, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n",
      "Cell 2 = 3.582729760737487\n",
      "{'a': 0, 'b': 0, 'c': 0, 'd': 0, 'e': 0, 'f': 0, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n",
      "Cell 3 = 4.08746284125034\n",
      "{'a': 0, 'b': 0, 'c': 0, 'd': 0, 'e': 0, 'f': 0, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 0, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'u': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0}\n",
      "Cell 4 = 3.582729760737487\n"
     ]
    }
   ],
   "source": [
    "################# code for section 3\n",
    "import math \n",
    "import itertools\n",
    "english_letters = ['a','z','e','r','t','y','u','i','o','p','q','s','d','f','g','h','j','k','l','m','w','x','c','v','b','n']\n",
    "\n",
    "def count_occurrences_in_position(words,pos):\n",
    "    counter = {english_letters[i]: 0 for i in range(len(english_letters))}\n",
    "    for word in words:\n",
    "        counter[word[pos]] = counter[word[pos]] + 1 \n",
    "    return counter\n",
    "\n",
    "def compute_entropy_of_cell(nb_words,counter):\n",
    "\n",
    "    entropy = 0\n",
    "    for letter in counter:\n",
    "        if counter[letter] == 0:\n",
    "            continue\n",
    "        proba_of_letter = counter[letter]/nb_words\n",
    "        entropy -= proba_of_letter * math.log2(proba_of_letter)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def print_entropy_of_cells(words):\n",
    "    if not words :\n",
    "        return 0\n",
    "\n",
    "    print('Entropy of each cell')\n",
    "    for i in range(len(words[0])):\n",
    "        counter = count_occurrences_in_position(words,i)\n",
    "        print('Cell ' + str(i) + ' = ' + str(compute_entropy_of_cell(len(words),counter)))\n",
    "\n",
    "def make_all_possible_5_words():\n",
    "    english_letters.sort()\n",
    "    all_words = []\n",
    "    for item in itertools.product(english_letters, repeat=5):\n",
    "        all_words.append(item)\n",
    "        \n",
    "    return all_words\n",
    "    \n",
    "def eliminate_words_by_letter_in_pos(words, letter, pos):\n",
    "    rest = []\n",
    "\n",
    "    for word in words:\n",
    "        if word[pos] != letter:\n",
    "            rest.append(word)\n",
    "\n",
    "    return rest\n",
    "\n",
    "def eliminate_words_by_letters_in_pos(words,letters,pos):\n",
    "    rest = words\n",
    "\n",
    "    for letter in letters:\n",
    "        rest = eliminate_words_by_letter_in_pos(rest,letter,pos)\n",
    "\n",
    "    return rest\n",
    "\n",
    "def eliminate_words_by_letters_different_to_letter(words,letter,pos):\n",
    "\n",
    "    left_letters = []\n",
    "    for l in english_letters:\n",
    "        if l != letter:\n",
    "            left_letters.append(l)\n",
    "\n",
    "    return eliminate_words_by_letters_in_pos(words,left_letters,pos)\n",
    "\n",
    "def compute_entropy_of_each_cell():\n",
    "    return\n",
    "\n",
    "\n",
    "def green_letters(words,letters,pos):\n",
    "    rest = words\n",
    "    for i in range(len(letters)):\n",
    "        rest = eliminate_words_by_letters_different_to_letter(words,letters[i],pos[i])\n",
    "    return rest\n",
    "\n",
    "def grey_letters(words,letters):\n",
    "    # Eliminates words if letters are present\n",
    "    return [elements for elements in words if all(ch not in elements for ch in letters)] \n",
    "\n",
    "def orange_letters(words,letters,pos):\n",
    "    rest = words\n",
    "    rest = [elements for elements in words if all(ch in elements for ch in letters)] # Eliminate words if letters not present\n",
    "\n",
    "    for i in range(len(letters)):\n",
    "        rest = eliminate_words_by_letter_in_pos(rest,letters[i],pos[i])\n",
    "    return rest\n",
    "\n",
    "words = make_all_possible_5_words()\n",
    "print(\"Number of different words at the start = \" + str(len(words))) \n",
    "print(\"Entropy = \" + str(math.log2(len(words))))\n",
    "\n",
    "### After first guess, tble are not in the word. letter in pos 1 = a\n",
    "words = grey_letters(words,['t','b','l','e'])\n",
    "words = green_letters(words,['a'],[1])\n",
    "\n",
    "print(\"Number of different words after first guess = \" + str(len(words))) \n",
    "print(\"Entropy = \" + str(math.log2(len(words))))\n",
    "\n",
    "### After second guess, rouh are not in the word, g is in the word but not in pos 3\n",
    "words = grey_letters(words, ['r','o','u','h'])\n",
    "words = orange_letters(words, ['g'],[3])\n",
    "\n",
    "print(\"Number of different words after second guess = \" + str(len(words))) \n",
    "print(\"Entropy = \" + str(math.log2(len(words))))\n",
    "print_entropy_of_cells(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
